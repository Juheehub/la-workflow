---
title: "Narrated: Foundations Case Study" 
subtitle: "Independent/Group work"
author: "PUT YOUR NAME HERE"
date: "`r format(Sys.Date(),'%B %e, %Y')`"
output:
  html_document:
    toc: yes
    toc_depth: '5'
    toc_float: yes
name: ''
editor_options:
  markdown:
    wrap: 72
---

## 0. INTRODUCTION

We will focus on online science classes provided through a state-wide
online virtual school and conduct an analysis that help product
students' performance in these online courses. This case study is guided
by a foundational study in Learning Analytics that illustrates how
analyses like these can be used develop an early warning system for
educators to identify students at risk of failing and intervene before
that happens.

Over the next labs we will dive into the Learning Analytics Workflow as
follows:

![](img/workflow.png){width="78%"}

Figure 1. Steps of Data-Intensive Research Workflow

1.  **Prepare**: Prior to analysis, it's critical to understand the
    context and data sources you're working with so you can formulate
    useful and answerable questions. You'll also need to become familiar
    with and load essential packages for analysis, and learn to load and
    view the data for analysis.
2.  **Wrangle**: Wrangling data entails the work of manipulating,
    cleaning, transforming, and merging data. In Part 2 we focus on
    importing CSV files, tidying and joining our data.
3.  **Explore**: In Part 3, we use basic data visualization and
    calculate some summary statistics to explore our data and see what
    insight it provides in response to our questions.
4.  **Model:** After identifying variables that may be related to
    student performance through exploratory analysis, we'll look at
    correlations and create some simple models of our data using linear
    regression.
5.  **Communicate:** To wrap up our case study, we'll develop our first
    "data product" and share our analyses and findings by creating our
    first web page using Markdown.
6.  **Change Idea:** Having developed a webpage using Markdown, share
    your findings with the colleagues. The page will include interactive
    plots and a detailed explanation of the analysis process, serving as
    a case study for other educators in your school. Present your
    findings at a staff meeting, advocating for a broader adoption of
    data-driven strategies across curriculums.

------------------------------------------------------------------------

## Module 1: Prepare and Wrangle

## 1. PREPARE

This case study is guided by a well-cited publication from two authors
that have made numerous contributions to the field of Learning Analytics
over the years. This article is focused on "early warning systems" in
higher education, and where adoption of learning management systems
(LMS) like Moodle and Canvas gained a quicker foothold.

Macfadyen, L. P., & Dawson, S. (2010). [Mining LMS data to develop an
"early warning system" for educators: A proof of
concept.](https://www.sciencedirect.com/science/article/pii/S0360131509002486?via%3Dihub)
*Computers & education*,Â *54*(2), 588-599.

#### ABOUT the study

Previous research has indicated that universities and colleges could
utilize Learning Management System (LMS) data to create reporting tools
that identify students who are at risk and enable prompt pedagogical
interventions. The present study validates and expands upon this idea by
presenting data from an international research project that explores the
specific online activities of students that reliably indicate their
academic success. This paper confirms and extends this proposition by
providing data from an international research project investigating
**which student online activities accurately predict academic
achievement.**

The **data analyzed** in this exploratory research was extracted from
the **course-based instructor tracking logs** and the **BB Vista
production server**.

Data collected on each student included 'whole term' counts for
frequency of usage of course materials and tools supporting content
delivery, engagement and discussion, assessment and
administration/management. In addition, tracking data indicating total
time spent on certain tool-based activities (assessments, assignments,
total time online) offered a total measure of individual student time on
task.

The authors used scatter plots for identifying potential relationships
between variables under investigation, followed by a a simple
correlation analysis of each variable to further interrogate the
significance of selected variables as indicators of student achievement.
Finally, a linear multiple regression analysis was conducted in order to
develop a predictive model in which a student final grade was the
continuous dependent variable.

+----------------------------------------------------------------------+
| **Introduction to the [Stakeholder]{.underline}**                    |
+======================================================================+
| **Name:** Alex Johnson                                               |
|                                                                      |
| **Role:** University Science Professor                               |
|                                                                      |
| **Experience:** 5 years teaching, enthusiastic about integrating     |
| technology in education                                              |
|                                                                      |
| **Goal:** Alex aims to improve student engagement and performance in |
| her online science classes.                                          |
+----------------------------------------------------------------------+

```{=html}
<style>
    .wrap-img {
        float: left;
        margin-right: 10px;
        width: 10%;
    }
</style>
```
<img src="img/teacher%20persona.png" alt="Teacher Persona" class="wrap-img"/>
***Alex begins by understanding the importance of data analysis in
identifying students who might need extra support. The cited
foundational study motivates her to explore similar analyses to develop
her own early warning system.***

### Load Libraries

Remember libraries are also called packages. They are shareable
collections of code that can contain functions, data, and/or
documentation and extend the functionality of the coding language.

-   `Tidyverse`: is a collection of R packages designed for data
    manipulation, visualization, and analysis.

```{r}
#Load Libraries below needed for analysis
library(tidyverse)
```

### Data Sources

#### Data Source #1: Log Data

Log-trace data is data generated from our interactions with digital
technologies, such as archived data from social media postings. In
education, an increasingly common source of log-trace data is that
generated from interactions with LMS and other digital tools.

The data we will use has already been "wrangled" quite a bit and is a
summary type of log-trace data: the number of minutes students spent on
the course. While this data type is fairly straightforward, there are
even more complex sources of log-trace data out there (e.g., time stamps
associated with when students started and stopped accessing the course).

**Variable Description:**

| Variable             | Description                                        |
|----------------------|----------------------------------------------------|
| `student_id`         | students id at institution                         |
| `course_id`          | abbreviation for \`course, course number, semester |
| `gender`             | m ale/female/NA                                    |
| `enrol lment_reason` | reason student decided to take the course          |
| `enrol lment_status` | ap pr ove/enrolled, dropped, withdrawn             |
| `time_spent`         | Time spent in hours for entire course              |

: Course Acronym description

-   "AnPhA" = "Anatomy",
-   "BioA" = "Biology",
-   "FrScA" = "Forensics",
-   "OcnA" = "Oceanography",
-   "PhysA" = "Physics"

### Data Source #2: Academic Achievement Data

**Variable Description:**

| Variable                 | Description                                 |
|--------------------------|---------------------------------------------|
| `total_p oints_possible` | available points for the course             |
| `total _points_earned`   | stud \| ent earned for the entire course \| |

#### Data Source #3: Self-Report Survey

The third data source is a self-report survey. This was data collected
before the start of the course. The survey included ten items, each
corresponding to one of three motivation measures: interest, utility
value, and perceived competence. These were chosen for their alignment
with one way to think about students' motivation, to what extent they
expect to do well (corresponding to their perceived competence) and
their value for what they are learning (corresponding to their interest
and utility value).

**Variable Description:**

| Var iable | Description              |
|-----------|--------------------------|
| `int`     | student science interest |
| `tv`      | `tv`                     |
| `Q1 -Q10` | survey questions         |

1.  I think this course is an interesting subject. (Interest)
2.  What I am learning in this class is relevant to my life. (Utility
    value)
3.  I consider this topic to be one of my best subjects. (Perceived
    competence)
4.  I am not interested in this course. (Interest---reverse coded)
5.  I think I will like learning about this topic. (Interest)
6.  I think what we are studying in this course is useful for me to
    know. (Utility value)
7.  I don't feel comfortable when it comes to answering questions in
    this area. (Perceived competence--reverse coded)
8.  I think this subject is interesting. (Interest)
9.  I find the content of this course to be personally meaningful.
    (Utility value)
10. I've always wanted to learn more about this subject. (Interest)

## 2. WRANGLE

### Import data

We will need to load in and inspect each of the dataframes that we will
use for this lab. You will first read about the dataframe and then learn
how to load (or read in) the dataframe into the quarto document.

#### **Time spent**

Let's use the `read_csv()` function from to import our `log-data.csv`
file directly from our data folder and name this data set `time_spent`,
to help us to quickly recollect what function it serves in this
analysis:

Load the file `log-data.csv` from data folder and save object as
`time_spent`.

#### Creating new variable

To do that, we need to create a new variable `time_spent` which is done
by naming the variable and assigning its value using `<-` operator.

Press the green arrow head to run the code below:

```{r}
#load log-data file from data folder
time_spent <- read_csv("module_1/data/log-data.csv")
time_spent
```

#### **Grades**

Load the file `gradebook-summary.csv` from data folder and save object
as `gradebook`

âï¸In R, everything is an **object**. An object can be a simple value
(like a number or a string), a complex structure (like a data frame or a
list), or even a function or a model. For example, when you load a CSV
file into R and store it in a variable, that variable is an object that
contains your dataset.

A **dataset** typically refers to a collection of data, often stored in
a tabular format with rows and columns.

##### **ð Your Turn** **â¤µ**

You need to:

1.  First, use the correct function to read in the .csv file and load
    the `gradebook-summary.csv` file.
2.  Second, add a function to the code (to inspect the data (your
    choice).
3.  Third, press the green arrow head to run the code.

```{r}
#load grade book data from data folder
#YOUR CODE HERE
gradebook <- read_csv("module_1/data/gradebook-summary.csv",show_col_types = FALSE)
```

#### Attitude survey

Load the file `survey.csv` from data folder.

##### **ð Your Turn** **â¤µ**

You need to:

1.  First, use the correct function to read in the .csv file and load
    the `survey.csv` file.
2.  Second, add a function to the code (to inspect the data (your
    choice).
3.  Third, press the green arrow head to run the code.

```{r}
#load survey data from data folder
#(add code below)
survey <- read_csv("module_1/data/survey.csv",show_col_types = FALSE)
```

### Inspect data

There are several ways you can look at data object in R and posit cloud.

#### Typing object name

Type the name of your object and run the code:

```{r}
time_spent
```

#### Using `glimpse()` function

```{r}
glimpse(gradebook)
```

#### Using Global Environment

![](img/global%20environment.png){width="34%"}

![](img/gradebook.png){width="50%"}

#### Inspecting first and last few rows

```{r}
#first few rows
head(survey)
```

```{r}
#last few rows
tail(survey)
```

#### Using `sample()` function

```{r}
sample(survey)
```

#### **ð Your Turn** **â¤µ**

Inspect three datasets we loaded and answer the question:

â What do you notice? What do you wonder about? Did you note the number
of observations, the different variables names? Finally what about the
classes the variables are such as numeric, integer, character, or
logical.

\*YOUR RESPONSE HERE

### Tidy data

#### 1. Time Spent

##### Use `separate()` function from `tidyr`

We will separate course_id variable in the time-spent.

The `c()` function in R is used used to combine or concatenate its
argument. You can use it to get the output by giving parameters inside
the function.

For example we want to separate `course_id` variables from

```{r}
#separate variable to individual subject, semester and section
time_spent %>%  
  separate(course_id,
           c("subject", "semester", "section"))

#inspect
time_spent
```

Make sure to save it to the `time_spent` object.

`Saving an object is accomplished by using an assignment operator, which looks kind of like an arrow (<-).`

```{r}
#separate variable to individual subject, semester and section and save as same object name
time_spent <- time_spent %>%  
  separate(course_id,
           c("subject", "semester", "section"))

#inspect
time_spent
```

##### Use `mutate()` function from `dplyr`

As you can see from the dataset, `time_spent` variable is *`not`* set as
hour.

Let's change that.

In pandas, you can easily create new variables or modify existing ones
in a DataFrame directly using column assignments.

```{r}
#mutate minutes to hours on time spent and save as new variable.
time_spent <- time_spent %>% 
  mutate(time_spent_hours = time_spent / 60)

#inspect 
time_spent
```

In R, you can create new variables in a dataset (data frame or tibble)
using the **`mutate()`** which allows you to add new columns to your
data frame or modify existing ones.

âï¸In this example, **`gradebook`** is the data frame, **`pass_fail`** is
the new variable, and **`if_else()`** is a function that assigns "Pass"
if the grade is greater than or equal to 50, and "Fail" otherwise.

#### 2. Gradebook

##### Use `separate()` function from `tidyr`

Now, we will work on the `gradebook` dataset. Like the previous dataset,
we will separate course_id variable again.

##### **ð Your Turn** **â¤µ**

You need to:

1.  First, use the pipe operator to separate `course_id` variable (like
    we just did in `time_spent`).
2.  Second, press the green arrow head to run the code.

```{r warning=FALSE, message=FALSE}
#separate the course_id variable and save to 'gradebook' object
#YOUR CODE HERE
gradebook <- gradebook %>% separate(course_id,
           c("subject", "semester", "section"))



#inspect
#YOUR CODE HERE
gradebook
```

##### Use the `mutate()` function from `dplyr`

As you can see in the gradebook dataframe the `total points earned` is
in points and it is hard to know the proportion. Therefore, we want it
to mutate that to a proportion.

##### **ð Your Turn** **â¤µ**

You need to:

1.  First, take `total_points_earned` divide by `total_points_possible`
    and multiple by 100. Save this as `proportion_earned`.
2.  Second, press the green arrow head to run the code.

```{r warning=FALSE, message=FALSE}

# Mutate to a proportion_earned, take 'total points earned' divide by 'total points possible.' Save as a new variable proportion_earned.
gradebook <- gradebook %>%
  mutate(proportion_earned = (total_points_earned /total_points_possible) *100)

#YOUR CODE HERE

#inspect data
gradebook
```

#### 3. Survey

Let's process our data. First though, take a quick look again by typing
`survey` into the console or using a preferred viewing method to take a
look at the data.

â Does it appear to be the correct file? What do the variables seem to
be about? What wrangling steps do we need to take? Taking a quick peak
at the data helps us to begin to formulate answers to these and is an
important step in any data analysis, especially as we *prepare* for what
we are going to do.

```{r warning=FALSE, message=FALSE}
#inspect data to view the column names
survey
```

ð¡ Look at the variable names.

##### **ð Answer below** **â¤µ**

Add one or more of the things you notice or wonder about the data here:

-   

-   

You may have noticed that `student_ID` is not formatted *exactly* the
same as `student_id` in our other files. This is important because in
the next section when we "join," or merge, our data files, these
variables will need to have identical names.

##### Use the `Janitor` package

Fortunately the
{[janitor](https://garthtarr.github.io/meatR/janitor.html)} package has
simple functions for examining and cleaning dirty data. It was built
with beginning and intermediate R users in mind and is optimized for
user-friendliness. There is also a handy function called `clean_names()`
in the {janitor} package for standardizing variable names.

##### **ð Your Turn** **â¤µ**

You need to:

1.  First, add `janitor` package using the `library` function.
2.  Second clean the columns by adding the `survey` object to the
    `clean_names()` functon and saving it to the `survey`object.
3.  Third, inspect the data.
4.  Fourth, press the green arrow head to run the code.

```{r warning=FALSE, message=FALSE}
# load janitor library to clean variable names that do not match
#YOUR CODE HERE
library(janitor)
```

```{r warning=FALSE, message=FALSE}
#clean columns of the survey data and save to survey object
#(add code below - some code is given to you)

survey <- clean_names(survey)

#inspect data to check for consistency with other data
#(add code below)

survey
```

### Data merging and reshaping

#### Join gradebook data with time spent datasets

We think, a `full_join` is best for our dataset as we need all the
information from all three datasets.

The full join returns all of the records in a new table, whether it
matches on either the left or right tables. If the table rows match,
then a join will be executed, otherwise it will return NULL in places
where a matching row does not exist.

When we are combining `gradebook1` and `time_spent` datasets, we should
identify column names. In this case, we will use the following variables
for the match:

-   `student_id`

-   `subject`

-   `semester`

-   `section`

```{r}
#use single join to join data sets by student_id, subject, semester and section.
joined_data <- full_join(gradebook, time_spent, 
                         by = c("student_id", "subject", "semester", "section"))

#inspect 
joined_data
```

As you can see, we have a new dataset, joined_data with 12
variables.Those variables came from the gradebook and time_spent
datasets.

#### Join survey dataset with joined dataset

#### Join Tables

As a reminder there are different joins. But we will mainly focus on
`full_join` for our dataset.

![](img/joins.png){width="467"}

Source:
<https://medium.com/>(imanjokko/data-analysis-in-r-series-vi-joining-data-using-dplyr-fc0a83f0f064)

Similar to what we learned in the code-a-long - combine the dataset
`joined_data` with `survey` dataset

##### **ð Your Turn** **â¤µ**

You need to:

1\. Use `full join` function to join `joined_data` with `survey` dataset
with the following variables:

-   `student_id`

-   `subject`

-   `semester`

-   `section`

2\. Save to a new object called `data_to_explore`.

3\. Inspect the data by clicking the green arrow head.

```{r eval = FALSE}
# use join to join data sets by student_id, subject, semester and section.
#(add code below - some code has been added for you)
#data_to_explore  <- full_join(survey, joined_data, 
                         #by = c("student_id", "subject", "semester", "section"))
  
#inspect
#data_to_explore
```

::: callout-note
**DON'T PANIC if you are getting an error - read below!!**
:::

Datasets cannot be joined because the class (type) of "student_id" is
different.

To fix this we need the same types of variables to join the datasets -
we will turn a numerical variable into a character variable.

##### **ð Answer below** **â¤µ**

â Check out what class student_id is in `joined_data` compared to
`survey` data. What do you notice? (HInt: think about the `class)`

\*YOUR ANSWER HERE:

##### Use `as.character()` function

##### **ð Your Turn** **â¤µ**

In the `joined_data` you may notice student_id is numerical so we also
need to rename have the unanimity in naming before we could join the
data.

You need to:

1.  First, use the `mutate` function and `as.character()` function to
    change `student_id` variable from numeric to character class.

2.  Save the new value to `student_id` variable.

3.  Finally, press the green arrow head to run the code.

```{r}
#mutate to change variable class from double or numeric to character
#(add code below - some code has been already added)
joined_data <- joined_data %>%
  mutate(student_id=as.character(student_id))
#survey <- subset(survey, select = -student_ID) #drop the column

#inspect
joined_data
```

##### NOW: - Full join

##### **ð Your Turn** **â¤µ**

Now, that the variables are the same class you need to:

1.  Use `full join` function to join `joined_data` with `survey` dataset
    with the following variables:

-   `student_id`

-   `subject`

-   `semester`

-   `section`

2.  Save to a new object called `data_to_explore`.
3.  Inspect the data by clicking the green arrow head

```{r}
#try again to together the grade_book and log_wrangled
#(add code below - some code has been already added)
data_to_explore  <- full_join(survey, joined_data, 
                         by = c("student_id", "subject", "semester", "section"))
  
#inspect
data_to_explore
```

Let's transform the subject names to a more convenient format:

```{r}
data_to_explore <- data_to_explore %>%
  mutate(subject = case_when(
    subject == "AnPhA"  ~ "Anatomy",
    subject == "BioA"   ~ "Biology",
    subject == "FrScA"  ~ "Forensics",
    subject == "OcnA"   ~ "Oceanography",
    subject == "PhysA"  ~ "Physics",
    TRUE ~ subject  #This line keeps the original value if none of the conditions above are met
  ))
data_to_explore
```

<img src="img/teacher%20persona.png" alt="Teacher Persona" class="wrap-img"/>
**Alex follows the steps to load and wrangle data, reflecting on how
each step can provide insights into her students' engagement levels. She
is particularly interested in understanding patterns in the time
students spend on different course materials and how these patterns
correlate with their performance.**

#### Filtering and sorting data

##### Use `filter()` function from {dplyr} package

We can identify students at risk of failing the course using the filter
function looking at students below 70:

```{r}
#Filter students with lower grades
at_risk_students <- data_to_explore %>%
  filter(proportion_earned<70)

#Print the at-risk students
at_risk_students
```

##### Use `arrange()` function from {dplyr} package to sort

```{r}
#sort in ascending order
data_to_explore %>%
  arrange(proportion_earned)
```

```{r}
#sort in descending order
data_to_explore %>%
  arrange(desc(proportion_earned))
```

##### **ð Your Turn** **â¤µ**

Think what other factors are important to identify students at risk. Run
your code and analyze the results:

```{r}
#YOUR CODE HERE:

```

#### Use `write_csv()` function

Now let's write the file to our **data folder** using the `write_csv()`
to save for later or download.

```{r eval=FALSE}
# add the function to write data to file to use later
write_csv(data_to_explore, "module_1/data/data_to_explore.csv")
```

Check the data folder to confirm the location of your new file.

### ð Stop here. Congratulations you finished the first part of the case study.

## 3. EXPLORE (Module 2)

### Exploratory Data Analysis

#### Use the `skimr` package

We've already wrangled out data - but let's look at the data frame to
make sure it is still correct. A quick way to look at the data frame is
with the [`skimr`
package](https://cran.r-project.org/web/packages/skimr/vignettes/skimr.html).

This output is best for internal use.This is because the output is rich,
but not well-suited to exporting to a table that you add, for instance,
to a Google Docs or Microsoft Word manuscript.

Of course, these values can be entered manually into a table, but we'll
also discuss ways later on to create tables that are ready, or
nearly-ready-to be added directly to manuscripts.

##### **ð Your Turn** **â¤µ**

You need to:

1.  First, load `skimr` package with the correct function.

Normally you would do this above but we want to make sure you know which
packages are used with the new functions.

```{r messages=FALSE, warning=FALSE}
#load library by adding skimr as the package name
#(add code below)
library(skimr)
```

##### **ð Your Turn** **â¤µ**

2.  Second, use the `skim()` function to view the `data_to explore`

```{r messages=FALSE, warning=FALSE}
#skim the data by adding the skim function in front of the data
#(add code below)
skim(data_to_explore)
```

In the code chunk below: us the `group_by` `subject` variable, then
`` skim()` ``

##### **ð Your Turn** **â¤µ**

3.  Third, use the `group_by()` function from `dplyr` with `subject`
    variable - then `skim()` function from `skimr` package.

```{r messages=FALSE, warning=FALSE}
data_to_explore %>%
  group_by(subject)%>%
  skim()
```

### Data Visualization

#### Use `ggplot2` package

GGplot is designed to work iteratively. You start with a layer that
shows the raw data. Then you add layers of annotations and statistical
summaries.

Remember GGPLOT is a part of the Tidyverse package so we do not need to
load it again.

You can read more about ggplot in the book ["GGPLOT: Elegant Graphics
for Data Analysis"](https://ggplot2-book.org/introduction.html). You can
also find lots of inspiration in the [r-graph
gallery](https://r-graph-gallery.com/) that includes code. Finally you
can use the GGPLOT cheat sheet to help.

![](module_4/img/grammar.png){width="382" height="340"}

"Elegant Graphics for Data Analysis" states that "every ggplot2 plot has
three key components:

-   data,

-   A set of aesthetic mappings between variables in the data and visual
    properties, and

-   At least one layer which describes how to render each observation.
    Layers are usually created with a geom function."

### One Continuous variable

Create a basic visualization that examines a continuous variable of
interest.

#### **Barplot**

We will be guided by the following research question.

â Which online course had the largest enrollment numbers?

â Which variable should we be looking at?

#### **ð Your Turn** **â¤µ**

You need to: 1. First, inspect the `data_to_explore` to understand what
variables we might need to explore the research question.

```{r messages=FALSE, warning=FALSE}
#inspect the data frame
#(add code below)
data_to_explore
```

#### Level a. The most basic level for a plot

As a reminder the most basic visualization that you can make with GGPLOT
include three things:

-   layer 1: **data**: data_to_explore.csv
-   layer 2: **`aes()` function** - one continuous variable:
    -   subject mapped to x position
-   layer 3: **Geom**:`geom_bar()` function - bar graph

```{r messages=FALSE, warning=FALSE}
#layer 1: add data 
# layer 2: add aesthetics mapping
ggplot(data_to_explore, aes(x = subject)) +
#layer 3: add geom 
  geom_bar() +
labs(title = "Number of Observations per Subject",
       x = "Subject",
       y = "Count")
```

#### Level b. Add another layer with labels

We can add the following to our ggplot visualization as layer 4

1.  **title**: "Number of Student Enrollments per Subject"

2.  **caption**: "Which online courses have had the largest enrollment
    numbers?"

```{r messages=FALSE, warning=FALSE}
#layer 1: add data 
# layer 2: add aesthetics mapping
ggplot(data_to_explore, aes(x = subject)) +
#layer 3: add geom 
  geom_bar() +

#layer 4: add labels
    labs(title = "Number of Student Enrollments per Subject",
       caption = "Which online courses have had the largest enrollment numbers?")
```

#### Level c: Add **Scale** with a different color.

We will be guided by the following research question.

â What can we notice about gender?

To answer the following research question we can add a scale layer:

-   layer 5: **scale**: fill = gender

```{r messages=FALSE, warning=FALSE}
#layer 1: add data 
# layer 2: add aesthetics mapping and #layer 5 scale
ggplot(data_to_explore, aes(x = subject, fill = gender)) +
#layer 3: add geom 
  geom_bar() +

#layer 4: add labels
    labs(title = "Gender Distribution of Students Across Subjects",
       caption = "Which subjects enroll more female students?")

```

#### **Histogram**

We will be guided by the following research question.

â What number is the number of hours students watch TV?

-   **data**: data_to_explore
-   **`aes()` function** - one continuous variables:
    -   `tv` variable mapped to x position
-   **Geom**: geom_histogram() *this code is already there you just need
    to un-comment it.*
-   Add a **title** "Number of Hours Students Watch TV per Day"
-   Add a **caption** that poses the question "Approximately how many
    students watch 4+ hours of TV per day?"

**NEED HELP? [TRY
STHDA](http://www.sthda.com/english/wiki/ggplot2-histogram-plot-quick-start-guide-r-software-and-data-visualization)**

#### **ð Your Turn** **â¤µ**

```{r messages=FALSE, warning=FALSE}
# Add data
ggplot(data_to_explore, aes(x = tv)) +
  # Add the geom
  geom_histogram(bins = 5) +
  # Add the labs
  labs(title = "Histogram of TV Watching Hours",
       x = "TV Watching Hours",
       y = "Count")
```

### Two categorical Variables

Create a basic visualization that examines the relationship between two
categorical variables.

We will be guided by the following research question.

â What do you wonder about the reasons for enrollment in various
courses?

#### **Heatmap**

-   **data**: data_to_explore
-   use `count()` function for `subject`, `enrollment` then,
-   `ggplot()` function
-   **`aes()` function** - one continuous variables
    -   `subject` variable mapped to x position
    -   `enrollment reason` variable mapped to x position
-   **Geom**: `geom_tile()` function
-   Add a **title** "Reasons for Enrollment by Subject"
-   Add a **caption**: "Which subjects were the least available at local
    schools?"

#### **ð Your Turn** **â¤µ**

```{r messages=FALSE, warning=FALSE}

data_to_explore %>%
  count(subject, enrollment_reason) %>%
  ggplot(aes(x = subject, y = enrollment_reason, fill = n)) + 
    geom_tile() +
    scale_fill_gradient(low = "orange", high = "maroon") +  #Change to red color gradient
    labs(title = "Reasons for Enrollment by Subject",
         caption = "Which subjects were the least available at local schools?",
         x = "Subject",
         y = "Enrollment Reason",
         fill = "Count")
```

### Two continuous variables

Create a basic visualization that examines the relationship between two
continuous variables.

#### **Scatter plot**

We will be guided by the following research question.

â Can we predict the grade on a course from the time spent in the
course LMS?

```{r message=FALSE, warning=FALSE}
#look at the data frame
 #(add code below)
head(data_to_explore)
```

â Which variables should we be looking at?

#### **ð Answer** here **â¤µ**

-   

#### Level a. The most basic level for a **scatter** plot

Includes:

-   **data**: data_to_explore.csv
-   **`aes()` function** - two continuous variables
    -   time spent in hours mapped to x position
    -   proportion earned mapped to y position
-   **Geom**: `geom_point()` function - Scatter plot

#### **ð Your Turn** **â¤µ**

```{r messages=FALSE, warning=FALSE}
#(add code below)
#layer 1: add data and aesthetics mapping 
ggplot(data_to_explore,
       aes(x = time_spent_hours, 
           y = proportion_earned)) +
#layer 2: +  geom function type
  geom_point()
```

#### Level b. Add another layer with labels

-   Add a **title**: "How Time Spent on Course LMS is Related to Points
    Earned in the course"
-   Add a **x label**: "Time Spent (Hours)"
-   Add a **y label**: "Proportion of Points Earned"

#### **ð Your Turn** **â¤µ**

```{r messages=FALSE, warning=FALSE}
# Look at the data frame
head(data_to_explore)

# Create a scatter plot
ggplot(data_to_explore, aes(x = time_spent_hours, y = proportion_earned)) +
  geom_point() +
  labs(title = "Relationship Between Time Spent and Proportion of Points Earned",
       x = "Time Spent (Hours)",
       y = "Proportion of Points Earned")
```

#### Level c. Add **Scale** with a different color.

#### â Can we notice anything about enrollment status?

-   Add **scale** in aes: color = enrollment_status

#### **ð Your Turn** **â¤µ**

```{r messages=FALSE, warning=FALSE}
# Create a scatter plot with color based on enrollment_status
ggplot(data_to_explore, aes(x = time_spent_hours, y = proportion_earned, color = enrollment_status)) +
  geom_point() +
  labs(title = "How Time Spent on Course LMS is Related to Points Earned in the course?",
       x = "Time Spent (Hours)",
       y = "Proportion of Points Earned",
       color = "Enrollment Status")
```

#### Level d. Divide up graphs using facet to visualize by subject.

-   Add **facet** with facet_wrap() function: by subject

#### **ð Your Turn** **â¤µ**

```{r message=FALSE, warning=FALSE}
# Create a scatter plot with facets for each subject
ggplot(data_to_explore, aes(x = time_spent_hours, y = proportion_earned, color = enrollment_status)) +
  geom_point() +
  facet_wrap(~subject) +
  labs(title = "Relationship Between Time Spent and Proportion of Points Earned by Subject",
       x = "Time Spent (Hours)",
       y = "Proportion of Points Earned",
       color = "Enrollment Status")
```

#### Level e. How can we remove NA's from plot? and What will the code look like without the comments?

-   use **data** then
-   add `enrollment status` to the `drop_na` function to remove na's
-   add labels to the `labs()` function like above.
-   Facet wrap by subject

```{r messages=FALSE, warning=FALSE}
# Drop rows with missing values and create the scatter plot
data_to_explore %>%
  drop_na(time_spent_hours, proportion_earned, enrollment_status, subject) %>%
  ggplot(aes(x = time_spent_hours, 
             y = proportion_earned, 
             color = enrollment_status)) +
  geom_point() +
  labs(title = "Relationship Between Time Spent and Proportion of Points Earned by Subject",
       x = "Time Spent (Hours)",
       y = "Proportion of Points Earned",
       color = "Enrollment Status") +
  facet_wrap(~subject)
```

<img src="img/teacher%20persona.png" alt="Teacher Persona" class="wrap-img"/>
***As Alex explores the data through visualizations and summary
statistics, she begins to see trends that could indicate which students
are at risk. Her observations guide her to consider changes in her
teaching approach or additional support for certain students.***

### ð Stop here. Congratulations you finished the second part of the case study.

## 4. Model (Module 3)

Quantify the insights using mathematical models. As highlighted
in.[Chapter 3 of Data Science in Education Using
R](https://datascienceineducation.com/c03.html), the.**Model** step of
the data science process entails "using statistical models, from simple
to complex, to understand trends and patterns in the data."

The authors note that while descriptive statistics and data
visualization during the**Explore**step can help us to identify patterns
and relationships in our data, statistical models can be used to help us
determine if relationships, patterns and trends are actually meaningful.

### Correlation Matrix

As highlighted in @macfadyen2010, scatter plots are a useful initial
approach for identifying potential correlational trends between
variables under investigation, but to further interrogate the
significance of selected variables as indicators of student achievement,
a simple correlation analysis of each variable with student final grade
can be conducted.

There are two efficient ways to create correlation matrices, one that is
best for internal use, and one that is best for inclusion in a
manuscript. The {corrr} package provides a way to create a correlation
matrix in a {tidyverse}-friendly way. Like for the {skimr} package, it
can take as little as a line of code to create a correlation matrix. If
not familiar, a correlation matrix is a table that presents how *all of
the variables* are related to *all of the other variables*.

#### **ð Your Turn** **â¤µ**

You need to:

1\. Load the `corrr` package using the correct function. (You may need
to install.packages() in th console if this is your first time using
loading the package.)

```{r messages=FALSE, warning=FALSE}
# load in corrr library
#(add code below)
library(corrr)
```

### Simple Correlation

#### **ð Your Turn** **â¤µ**

Look and see if there is a simple correlation between by:

You need to:

1\. use `data_to_explore`

2\. `select()`: - `time-spent-hours` - `proportion_earned`

3\. use `correlate()` function

```{r messages = FALSE, warning=FALSE}
#(add code below)
data_to_explore %>% 
  select(proportion_earned, time_spent_hours) %>%
  correlate()
```

**For printing** purposes,the `fashion()` function can be added for
converting a correlation data frame into a matrix with the correlations
cleanly formatted (leading zeros removed; spaced for signs) and the
diagonal (or any NA) left blank.

```{r messages=FALSE, warning=FALSE}
#add fashion function
data_to_explore %>% 
  select(proportion_earned, time_spent_hours) %>% 
  correlate() %>% 
  rearrange() %>%
  shave() %>%
  fashion()
```

â What could we write up for a manuscript in APA format or another
format? Write below **â¤µ**

-   *In the study, Pearson's correlation coefficient was calculated to
    assess the relationship between the proportion of course materials
    earned and the time spent in hours. The analysis revealed a moderate
    positive correlation of .44 between these variables, suggesting that
    as the time students spent on course materials increased, so did
    their proportion of earned materials (pairwise complete observations
    were used to handle missing data).*

â What other variables would you like to check out? Write below **â¤µ**

-   

#### **ð Your Turnâ¤µ**

```{r messages=FALSE, warning=FALSE}
#(add code below)

```

### APA Formatted Table

While {corrr} is a nice package to quickly create a correlation matrix,
you may wish to create one that is ready to be added directly to a
dissertation or journal article. {apaTables} is great for creating more
formal forms of output that can be added directly to an APA-formatted
manuscript; it also has functionality for regression and other types of
model output. It is not as friendly to {tidyverse} functions; first, we
need to select only the variables we wish to correlate.

Then, we can use that subset of the variables as the argument to
the`apa.cor.table()` function.

Run the following code to create a subset of the larger
`data_to_explore` data frame with the variables you wish to correlate,
then create a correlation table using `apa.cor.table()`.

-   load `apaTables` library

#### **ð Your Turn** **â¤µ**

```{r messages=FALSE, warning=FALSE}
# read in apatables library
#(add code below)
library(apaTables)

data_to_explore_subset <- data_to_explore %>% 
  select(time_spent_hours, proportion_earned, int)

apa.cor.table(data_to_explore_subset)
```

This may look nice, but how to actually add this into a dissertation or
article that you might be interested in publishing?

Read the documentation for `apa.cor.table()` by running
`?apa.cor.table()` in the console. Look through the documentation and
examples to understand how to output a file with the formatted
correlation table, and then run the code to do that with your subset of
the `data_to_explore` data frame.

```{r messages=FALSE, warning=FALSE}
apa.cor.table(data_to_explore_subset, filename = "cor-table.doc")
```

You should now see a new Word document in your project folder called
`survey-cor-table.doc`. Click on that and you'll be prompted to download
from your browser.

##### C. Predict Academic Achievement

##### Linear Regression

In brief, a linear regression model involves estimating the
relationships between one or more *independent variables* with one
dependent variable. Mathematically, it can be written like the
following.

$$
\operatorname{dependentvar} = \beta_{0} + \beta_{1}(\operatorname{independentvar}) + \epsilon
$$

Does time spent predict grade earned?

The following code estimates a model in which `proportion_earned`, the
proportion of points students earned, is the dependent variable. It is
predicted by one independent variable

-   Add + `int`, after `time_spent_hours` for students' self-reported
    interest in science.

#### **ð Your Turn** **â¤µ**

```{r messages=FALSE, warning=FALSE}

# add predictor variable for `science interest `int`
lm(proportion_earned ~ time_spent_hours , 
   data = data_to_explore)
```

We can see that the intercept is now estimated at 0.44, which tells us
that when students' time spent and interest are equal to zero, they are
likely fail the course unsurprisingly. Note that that estimate for
interest in science is .046, so for every one-unit increase in `int`, we
should expect an 5 percentage point increase in their grade.

We can save the output of the function to an object---let's say `m1`,
standing for model 1. We can then use the `summary()` function built
into R to view a much more feature-rich summary of the estimated model.

```{r messages=FALSE, warning=FALSE}
# save the model
m1 <- lm(proportion_earned ~ time_spent_hours + int, data = data_to_explore)

```

Run a summary model for the model you just created called, `m1.`

D. Assumptions

Great! Now that you have defined your linear model `m1` in R, which
predicts `proportion_earned` based on `time_spent_hours` and the
interest variable `int`, let's go through how to check the assumptions
of this linear model using the various diagnostic plots and tests.

We'll need to check:

1.  Linearity and Interaction Effects

2.  Residuals Analysis

3.  Normality of Residuals

4.  Multicollinearity

### Linearity and Interaction effect

Since our model includes an interaction term (`int`), it's good to first
check if the interaction is meaningful and whether the linearity
assumption holds for the predictors in relation to the dependent
variable.

```{r}
ggplot(data_to_explore, aes(x=time_spent_hours, y=proportion_earned, color=int)) +
  geom_point() +
  geom_smooth(method="lm", se=FALSE)
```

This plot helps visualize if the interaction term significantly affects
the relationship between your predictors and the dependent variable.

### Residuals Analysis

Next, plot the residuals against the fitted values to check for
independence, homoscedasticity, and any unusual patterns.

Residuals vs. Fitted Values Plot (for homoscedasticity and independence

```{r}
plot(residuals(m1) ~ fitted(m1))
abline(h = 0, col = "red")
```

Look for a random dispersion of points. Any pattern or funnel shape
indicates issues with homoscedasticity or linearity.

### Normality of Residuals

Normal Q-Q Plot (for normality of residuals)

```{r}
qqnorm(residuals(m1))
qqline(residuals(m1))
```

A deviation from the straight line in the Q-Q plot indicates deviations
from normality.

Shapiro-Wilk Test:

```{r}
shapiro.test(residuals(m1))
```

-   W statistic: The test statistic W is 0.88967. This value indicates
    the extent to which the data are normally distributed. W values
    close to 1 suggest that the data closely follow a normal
    distribution. In this case, a W value of 0.88967 suggests some
    deviation from normality.

-   p-value: The p-value is less than 2.2e-16 (a very small number close
    to zero). In statistical testing, a p-value less than the chosen
    significance level (typically 0.05) leads to the rejection of the
    null hypothesis.

### Multicollinearity

```{r}
#load the package car
library(car)
vif(m1)
```

### Interpretation of VIF Scores:

-   **VIF = 1**: No correlation among the predictor and all other
    predictors.

-   **VIF \< 5**: Generally indicates a moderate level of
    multicollinearity.

-   **VIF \>= 5 to 10**: May indicate a problematic amount of
    multicollinearity, depending on the context and sources.

-   **VIF \> 10**: Signifies high multicollinearity that can severely
    distort the least squares estimates.

Both of the VIF scores are slightly above 1, which suggests that there
is almost no multicollinearity among these predictors. This is a good
sign, indicating that each predictor provides unique and independent
information to the model, not unduly influenced by the other variables.

Our model was violated and does not follow a normality. However, we are
going to practice other functions as if it passed all assumptions. THe
assumption testing is just for you to know how to do in the future. You
will need to correct issues like normality hby exploring data
transformations, adding polynomial or interaction terms to the model, or
using a different type of regression model that does not assume
normality of residuals.

#### **ð Your Turn** **â¤µ**

```{r messages=FALSE, warning=FALSE}
#run the summary
summary(m1)

```

Let's save this as a nice APA table for possible publication

```{r messages=FALSE, warning=FALSE}
# use the {apaTables} package to create a nice regression table that could be used for later publication.
apa.reg.table(m1, filename = "lm-table.doc")
```

<img src="img/teacher%20persona.png" alt="Teacher Persona" class="wrap-img"/>
***By creating simple models, Alex hopes to predict student outcomes
more accurately. She is interested in how variables like time spent on
tasks correlate with student grades and uses this information to adjust
her instructional strategies.***

#### Summarize predictors

The `summarize()` function from the {dplyr} package used to create
summary statistics such as the mean, standard deviation, or the minimum
or maximum of a value.

At its core, think of `summarize()` as a function that returns a single
value (whether it's a mean, median, standard deviation---whichever!)
that summarizes a single column.

In the space below find the mean interest of students. `summarize()`

```{r messages=FALSE, warning=FALSE}
data_to_explore %>% 
  summarize(mean_interest = mean(int, na.rm = TRUE))
```

Now let's look at the mean of `time_spent_hours` and remove any NA's. -
save it to a new variable called `mean_time`

#### **ð Your Turn** **â¤µ**

```{r message=FALSE, warning=FALSE}
#add code here
data_to_explore %>% 
  summarize(mean_time = mean(time_spent_hours, na.rm = TRUE))
```

The mean value for interest is quite high. If we multiply the estimate
relationship between interest and proportion of points
earned---0.046---by this, the mean interest across all of the
students---we can determine that students' estimate final grade was
0.046 X 4.3, or **0.197**. For hours spent spent, the average students'
estimate final grade was 0.0042 X 30.48, or **0.128**.

If we add both 0.197 and 0.128 to the intercept, 0.449, that equals
0.774, or about 77%. In other words, a student with average interest in
science who spent an average amount of time in the course earned a
pretty average grade.

## Communicate

For your final Your Turn, your goal is to distill our analysis into a
FLEXBOARD "data product" designed to illustrate key findings. Feel free
to use the template in the lab 4 folder.

The final step in the workflow/process is sharing the results of your
analysis with wider audience. Krumm et al. @krumm2018 have outlined the
following 3-step process for communicating with education stakeholders
findings from an analysis:

1.  **Select.**Â Communicating what one has learned involves selecting
    among those analyses that are most important and most useful to an
    intended audience, as well as selecting a form for displaying that
    information, such as a graph or table in static or interactive form,
    i.e.Â a "data product."

2.  **Polish**. After creating initial versions of data products,
    research teams often spend time refining or polishing them, by
    adding or editing titles, labels, and notations and by working with
    colors and shapes to highlight key points.

3.  **Narrate.**Â Writing a narrative to accompany the data products
    involves, at a minimum, pairing a data product with its related
    research question, describing how best to interpret the data
    product, and explaining the ways in which the data product helps
    answer the research question and might be used to inform new
    analyses or a "change idea" for improving student learning.

#### **ð Your Turn** **â¤µ**

Create a Data Story with our current data set. Make sure to use the LA
workflow as your guid to include

\- Develop a research question

\- Add ggplot visualizations

\- Modeling visualizations

\- Communicate by writing up a short write up for the intended
stakeholders. Remember to write it in terms the stakeholders understand.

<img src="img/teacher%20persona.png" alt="Teacher Persona" class="wrap-img"/>
***Finally, Alex prepares to communicate her findings. She creates a
simple web page using Markdown to share her insights with colleagues.
This acts as a practical example of how data can inform teaching
practices.***
